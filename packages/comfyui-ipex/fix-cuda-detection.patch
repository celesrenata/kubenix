--- a/comfy/model_management.py
+++ b/comfy/model_management.py
@@ -185,7 +185,12 @@
         elif is_mlu():
             return torch.device("mlu", torch.mlu.current_device())
         else:
-            return torch.device(torch.cuda.current_device())
+            try:
+                if torch.cuda.is_available():
+                    return torch.device(torch.cuda.current_device())
+            except RuntimeError:
+                pass
+            return torch.device("cpu")
 
 def get_total_memory(dev=None, torch_total_too=False):
     global directml_enabled
@@ -218,7 +223,11 @@
             mem_total_torch = mem_reserved
             mem_total = mem_total_mlu
         else:
-            stats = torch.cuda.memory_stats(dev)
+            try:
+                stats = torch.cuda.memory_stats(dev)
+            except (RuntimeError, AttributeError):
+                # CUDA not available, fall back to system memory
+                return psutil.virtual_memory().total
             mem_reserved = stats['reserved_bytes.all.current']
             _, mem_total_cuda = torch.cuda.mem_get_info(dev)
             mem_total_torch = mem_reserved
